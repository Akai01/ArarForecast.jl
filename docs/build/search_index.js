var documenterSearchIndex = {"docs":
[{"location":"#ArarForecast.jl","page":"Home","title":"ArarForecast.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Forecasting using Arar Algorithm","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"ArarForecast\")\n    \n # dev version\nPkg.add(url = \"https://github.com/Akai01/ArarForecast.jl.git\")","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"#Load-packages","page":"Home","title":"Load packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using CSV\nusing Downloads\nusing DataFrames\nusing TimeSeries\nusing Dates\nusing ArarForecast","category":"page"},{"location":"#Load-the-data","page":"Home","title":"Load the data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"dta = CSV.File(Downloads.download(\"https://raw.githubusercontent.com/Akai01/example-time-series-datasets/main/Data/AirPassengers.csv\")) |> DataFrame;","category":"page"},{"location":"#Create-a-TimeArray","page":"Home","title":"Create a TimeArray","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"data = (date = dta[:,\"ds\"], data = dta[:, \"y\"]);\ndata = TimeArray(data; timestamp = :date);","category":"page"},{"location":"","page":"Home","title":"Home","text":"There are different ways to create a TimeArray see TimeSeries.jl package.","category":"page"},{"location":"#Forecasting","page":"Home","title":"Forecasting","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"fc = arar(data, 12, Month)\n\n## 12×5 TimeArray{Float64, 2, Date, Matrix{Float64}} 1961-01-31 to 1961-12-31\n## │            │ Point_Forecast │ Upper95  │ Upper80  │ Lower95  │ Lower80  │\n## ├────────────┼────────────────┼──────────┼──────────┼──────────┼──────────┤\n## │ 1961-01-31 │ 466.1915       │ 486.7582 │ 479.6228 │ 445.6248 │ 452.7602 │\n## │ 1961-02-28 │ 426.3592       │ 449.5853 │ 441.5272 │ 403.1331 │ 411.1912 │\n## │ 1961-03-31 │ 463.614        │ 489.4384 │ 480.4789 │ 437.7895 │ 446.749  │\n## │ 1961-04-30 │ 509.5108       │ 536.8182 │ 527.3442 │ 482.2035 │ 491.6775 │\n## │ 1961-05-31 │ 516.2016       │ 544.5864 │ 534.7386 │ 487.8169 │ 497.6647 │\n## │ 1961-06-30 │ 594.0837       │ 623.2017 │ 613.0995 │ 564.9658 │ 575.0679 │\n## │ 1961-07-31 │ 693.9735       │ 723.6112 │ 713.3287 │ 664.3358 │ 674.6182 │\n## │ 1961-08-31 │ 670.4816       │ 700.4859 │ 690.0762 │ 640.4772 │ 650.8869 │\n## │ 1961-09-30 │ 564.4617       │ 594.727  │ 584.2268 │ 534.1964 │ 544.6966 │\n## │ 1961-10-31 │ 518.5135       │ 549.7526 │ 538.9145 │ 487.2743 │ 498.1124 │\n## │ 1961-11-30 │ 434.7389       │ 465.992  │ 455.1491 │ 403.4857 │ 414.3287 │\n## │ 1961-12-31 │ 485.5744       │ 516.8683 │ 506.0112 │ 454.2805 │ 465.1376 │","category":"page"},{"location":"","page":"Home","title":"Home","text":"That’s it. It is easy to use and fast and the accuracy is comparable with ARIMA or Prophet. No hyper-parameter tuning needed.","category":"page"},{"location":"#How-does-the-ARAR-algorithm-Work?","page":"Home","title":"How does the ARAR algorithm Work?","text":"","category":"section"},{"location":"#Memory-Shortening","page":"Home","title":"Memory Shortening","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The ARAR algorithm applies a memory-shortening transformation if the underlying process of a given time series Y_t t = 1 2  n is \"long-memory\" then it fits an autoregressive model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The algorithm follows five steps to classify Y_t and take one of the following three actions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"L: declare Y_t as long memory and form Y_t by   tildeY_t = Y_t - hatphiY_t - hattau\nM: declare Y_t as moderately long memory and form Y_t by   tildeY_t = Y_t - hatphi_1Y_t -1 - hatphi_2Y_t -2\nS: declare Y_t as short memory.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If Y_t declared to be L or M then the series Y_t is transformed again until. The transformation process continuous until the transformed series is classified as short memory. However, the maximum number of transformation process is three, it is very rare a time series require more than 2 \\cite{ITSM}.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The algorithm:","category":"page"},{"location":"","page":"Home","title":"Home","text":"For eachtau = 1 2  15, we find the value hatphi(tau)  of \\hat{\\phi} that minimizes  ERR(phi tau) = fracsum_t=tau +1 ^n Y_t - phi Y_t-tau^2 sum_t=tau +1 ^n Y_t^2  then define Err(tau) = ERR(hatphi(tau) tau) and choose the  lag hattau to be the value of tau that minimizes  Err(tau).\nIf Err(hattau) leq 8n, Y_t is a long-memory series.\nIf hatphi( hattau ) geq 093 and hattau  2,  Y_t is a long-memory series.\nIf hatphi( hattau ) geq 093 and hattau = 1 or  2,Y_t is a long-memory series.\nIf hatphi( hattau )  093, Y_t is a short-memory  series","category":"page"},{"location":"#Subset-Autoregressive-Model","page":"Home","title":"Subset Autoregressive Model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the following we will describe how ARAR algorithm fits an autoregressive process to the mean-corrected series X_t = S_t- barS, t = k+1  n where S_t t = k + 1  n is the memory-shortened version of Y_t which derived from the five steps we described above and barS is the sample mean of S_k+1  S_n.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The fitted model has the following form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"X_t = phi_1Xt-1 + phi_1X_t-l_1 + phi_1X_t- l_1 + phi_1X_t-l_1 + Z","category":"page"},{"location":"","page":"Home","title":"Home","text":"where Z sim WN(0 sigma^2). The coefficients phi_j and white noise variance sigma^2 can be derived from the Yule-Walker equations for given lags l_1 l_2 and l_3:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginequation*\nbeginbmatrix\n1  hatrho(l_1 - 1)  hatrho(l_2 - 1)  hatrho(l_3 - 1)\nhatrho(l_1 - 1) 1  hatrho(l_2 - l_1)  hatrho(l_3 - l_1)\nhatrho(l_2 - 1)  hatrho(l_2 - l_1)  1  hatrho(l_2 - l_2)\nhatrho(l_3 - 1)  hatrho(l_3 - l_1)  hatrho(l_3 - l_1)  1\nendbmatrix\nbeginbmatrix\nphi_1 \nphi_l_1 \nphi_l_2\nphi_l_3\nendbmatrix = \nbeginbmatrix\nhatrho(1) \nhatrho(l_1) \nhatrho(l_2)\nhatrho(l_3)\nendbmatrix\nendequation*","category":"page"},{"location":"","page":"Home","title":"Home","text":"and sigma^2 = hatgamma(0) 1-phi_1hatrho(1) - phi_l_1hatrho(l_1) - phi_l_2hatrho(l_2) - phi_l_3hatrho(l_3), where hatgamma(j) and hatrho(j) j = 0 1 2  are the sample autocovariances and autocorelations of the series X_t.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The algorithm computes the coefficients of phi(j) for each set of lags where 1l_1l_2l_3 leq m where m chosen to be 13 or 26. The algorithm selects the model that the Yule-Walker estimate of sigma^2 is minimal.","category":"page"},{"location":"#Forecasting-2","page":"Home","title":"Forecasting","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If short-memory filter found in first step it has coefficients Psi_0 Psi_1  Psi_k (k geq0) where Psi_0 = 1. In this case the transforemed series can be expressed as ","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginequation*\nS_t = Psi(B)Y_t = Y_t + Psi_1 Y_t-1 + + Psi_k Y_t-k\n*endequation* ","category":"page"},{"location":"","page":"Home","title":"Home","text":"where Psi(B) = 1 + Psi_1B + + Psi_k B^k is polynomial in the back-shift operator.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If the coefficients of the subset autoregression found in the second step it has coefficients phi_1 phi_l_1 phi_l_2 and phi_l_3 then the subset AR model for X_t = S_t - barS is","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginequation*\nphi(B)X_t = Z_t\nendequation*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where Z_t is a white-noise series with zero mean and constant variance and phi(B) = 1 - phi_1B - phi_l_1B^l_1 - phi_l_2B^l_2 - phi_l_3B^l_3. From equation (1) and (2) one can obtain","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginequation*\n\nxi(B)Y_t = phi(1)barS + Z_t\n\nendequation*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where xi (B) = Psi(B)phi(B).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Assuming the fitted model in equation (3) is an appropriate model, and Z_t is uncorrelated with Y_j j tforall t in T, one can determine minimum mean squared error linear predictors P_n Y_n + hofY_n+hin terms of1 Y_1  Y_n for n  k + l_3, from recursions","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginequation*\nP_n Y_n+h = - sum_j = 1^k + l_3 xi P_nY_n+h-j + phi(1)barS  hgeq 1\nendequation* ","category":"page"},{"location":"","page":"Home","title":"Home","text":"with the initial conditions P_n Y_n+h = Y_n + h for hleq0","category":"page"},{"location":"#Ref:-Brockwell,-Peter-J,-and-Richard-A.-Davis.-Introduction-to-Time-Series-and-Forecasting.-[Springer](https://link.springer.com/book/10.1007/978-3-319-29854-2)-(2016)","page":"Home","title":"Ref: Brockwell, Peter J, and Richard A. Davis. Introduction to Time Series and Forecasting. Springer (2016)","text":"","category":"section"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Automatic model selection\nAutomatic Forecasting\nError maesurement","category":"page"},{"location":"#Function-Documentation","page":"Home","title":"Function Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"arar","category":"page"},{"location":"#ArarForecast.arar","page":"Home","title":"ArarForecast.arar","text":"arar(y::TimeArray, h::Int, freq::DataType, max_lag::Int)\n\nForecasting using ARAR algorithm.\n\nReturn A matrix of forecast values and prediction intervals\n\nArguments\n\ny::TimeArray: An TimeArray with only one value column.\nh::Int: Forecast horizon as an integer.\nfreq::DataType: A DataType from Dates, e.g. Dates.Day or Dates.Month.\nmax_lag::Int: An integer (>= 26) to specify maximum number of lags for sample autocovariance.\n\nExamples\n\njulia> arar(data, 12, Month)\n\n\n\n\n\n\n","category":"function"}]
}
